{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import datetime\n",
    "import numpy\n",
    "import scipy.optimize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import bs4\n",
    "import urllib\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import ticker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup some cutoff values we'll use in the analysis\n",
    "velCutoffUpper = 2000.\n",
    "velCutoffLower = 0.\n",
    "numPointsCutoffMLTMLAT = 25\n",
    "perCutoffMLTMLAT = 0.15\n",
    "mlatCutOffUpper = 65.\n",
    "mlatCutOffLower = 53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# READ Dst and AE data\n",
    "inpDstFile = \"../data/dst_out_file.csv\"\n",
    "dstDF = pandas.read_csv(inpDstFile, sep=' ',\\\n",
    "                infer_datetime_format=True,\\\n",
    "                        parse_dates=[\"dst_date\"])\n",
    "dstDF = dstDF[ (dstDF[\"dst_date\"] > datetime.datetime(2010,12,31)) &\\\n",
    "             (dstDF[\"dst_date\"] < datetime.datetime(2015,1,1))].reset_index(drop=True)\n",
    "dstDF = dstDF[ dstDF[\"dst_index\"] <= 10. ].reset_index(drop=True)\n",
    "dstDF[\"dtStr\"] = dstDF[\"dst_date\"].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "dstDF[\"hour\"] = dstDF[\"dst_date\"].apply(lambda x: x.strftime('%H'))\n",
    "# Aur Inds\n",
    "aurDF = pandas.read_csv( \"../data/aur_processed.txt\", sep=' ' )\n",
    "aurDF[\"date\"] = pandas.to_datetime(aurDF[\"datetimeStr\"], format='%Y%m%d-%H-%M')\n",
    "aurDF[\"hour\"] = aurDF[\"date\"].apply(lambda x: x.strftime('%H'))\n",
    "aurDF[\"minute\"] = aurDF[\"date\"].apply(lambda x: x.strftime('%M'))\n",
    "aurDF[\"dtStr\"] = aurDF[\"date\"].apply(lambda x: x.strftime('%Y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### In this block we load Velocity data ####\n",
    "#### In this block we load Velocity data ####\n",
    "#### In this block we load Velocity data ####\n",
    "# a helper function to convert seperate date\n",
    "# and time strings to datetime objects  \n",
    "fitVelFile = \"../data/fitres-extra.csv\"\n",
    "inpColNames = [\"azim\", \"azimStd\", \"delMLT\", \"endPtMLAT\",\\\n",
    "               \"endPtNormMLT\",\"goodFit\", \"MLAT\", \"normMLT\",\\\n",
    "               \"vSaps\", \"velSTD\", \"date\"]\n",
    "if \"extra\" in fitVelFile:\n",
    "    inpColNames = [\"azim\", \"azimStd\", \"delMLT\", \"goodFit\",\\\n",
    "               \"MLAT\", \"normMLT\", \"vSaps\", \"velSTD\",\\\n",
    "               \"endPtMLAT\", \"endPtNormMLT\", \"date\"]\n",
    "# velsDataDF = pandas.read_csv(fitVelFile, sep=' ', header=None)\n",
    "# velsDataDF.columns = inpColNames\n",
    "velsDataDF = pandas.read_csv(fitVelFile, sep=' ',\\\n",
    "                             header=None, names=inpColNames,\\\n",
    "                            infer_datetime_format=True,\\\n",
    "                            parse_dates=[\"date\"])\n",
    "\n",
    "velsDataDF[\"dtStr\"] = velsDataDF[\"date\"].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "# Discard unwanted values\n",
    "# We'll only consider those velocities \n",
    "# which lie between 0 and 2500 m/s\n",
    "# and located below 70 MLAT\n",
    "velsDataDF = velsDataDF[ (velsDataDF[\"vSaps\"] > velCutoffLower) \\\n",
    "                        & (velsDataDF[\"vSaps\"] < velCutoffUpper)\\\n",
    "                       ].reset_index(drop=True)\n",
    "velsDataDF = velsDataDF[ (velsDataDF[\"MLAT\"] < mlatCutOffUpper) &\\\n",
    "                       (velsDataDF[\"MLAT\"] > mlatCutOffLower) ].reset_index(drop=True)\n",
    "velsDataDF[\"hour\"] = velsDataDF[\"date\"].apply(lambda x: x.strftime('%H'))\n",
    "velsDataDF[\"minute\"] = velsDataDF[\"date\"].apply(lambda x: x.strftime('%M'))\n",
    "# Now merge the dst and velocity DFs\n",
    "velsDataDF = pandas.merge( velsDataDF, dstDF,\\\n",
    "                          on=[\"dtStr\", \"hour\"], how='inner' )\n",
    "# We generally work with Dst bins, set them up\n",
    "# add dst_bins\n",
    "dstBins = [ -150, -75, -50, -25, -10, 10 ]\n",
    "velsDataDF = pandas.concat( [ velsDataDF, \\\n",
    "                    pandas.cut( velsDataDF[\"dst_index\"], \\\n",
    "                               bins=dstBins ) ], axis=1 )\n",
    "velsDataDF.columns = ['azim', 'azimStd', 'delMLT', 'endPtMLAT', 'endPtNormMLT',\\\n",
    "                      'goodFit', 'MLAT', 'normMLT', 'vSaps', 'velSTD', 'date',\\\n",
    "                      'dtStr', 'hour', 'minute', 'dst_date', 'dst_index', 'dst_bin']\n",
    "if \"extra\" in fitVelFile:\n",
    "    velsDataDF.columns = [\"azim\", \"azimStd\", \"delMLT\", \"goodFit\",\\\n",
    "               \"MLAT\", \"normMLT\", \"vSaps\", \"velSTD\",\\\n",
    "               \"endPtMLAT\", \"endPtNormMLT\", \"date\",\\\n",
    "                'dtStr', 'hour', 'minute', 'dst_date',\\\n",
    "                'dst_index', 'dst_bin']\n",
    "# Also merge with aurDF\n",
    "# print \"pre merge shape-->\", velsDataDF.shape\n",
    "velsDataDF = pandas.merge( velsDataDF, aurDF,\\\n",
    "                         on=[\"dtStr\", \"hour\", \"minute\"], how='inner')\n",
    "# Discard some unwanted cols\n",
    "selColsVels = ['azim', 'azimStd', 'delMLT', 'endPtMLAT', 'endPtNormMLT',\\\n",
    "               'goodFit', 'MLAT', 'normMLT', 'vSaps', 'velSTD', 'date_x',\\\n",
    "               'dtStr', 'hour', 'minute', 'dst_date', 'dst_index', 'dst_bin',\\\n",
    "               'datetimeStr', 'AE', 'AL', 'AO', 'AU']\n",
    "if \"extra\" in fitVelFile:\n",
    "    selColsVels = [\"azim\", \"azimStd\", \"delMLT\", \"goodFit\",\\\n",
    "               \"MLAT\", \"normMLT\", \"vSaps\", \"velSTD\",\\\n",
    "               \"endPtMLAT\", \"endPtNormMLT\", \"date_x\",\\\n",
    "                'dtStr', 'hour', 'minute', 'dst_date',\\\n",
    "                'dst_index', 'dst_bin', 'datetimeStr', 'AE', 'AL', 'AO', 'AU']\n",
    "velsDataDF = velsDataDF[ selColsVels ]\n",
    "velsDataDF.columns = ['azim', 'azimStd', 'delMLT', 'endPtMLAT', 'endPtNormMLT',\\\n",
    "               'goodFit', 'MLAT', 'normMLT', 'vSaps', 'velSTD', 'date',\\\n",
    "               'dtStr', 'hour', 'minute', 'dst_date', 'dst_index', 'dst_bin',\\\n",
    "               'datetimeStr', 'AE', 'AL', 'AO', 'AU']\n",
    "if \"extra\" in fitVelFile:\n",
    "    velsDataDF.columns = [\"azim\", \"azimStd\", \"delMLT\", \"goodFit\",\\\n",
    "               \"MLAT\", \"normMLT\", \"vSaps\", \"velSTD\",\\\n",
    "               \"endPtMLAT\", \"endPtNormMLT\", \"date\",\\\n",
    "                'dtStr', 'hour', 'minute', 'dst_date',\\\n",
    "                'dst_index', 'dst_bin', 'datetimeStr', 'AE', 'AL', 'AO', 'AU']\n",
    "\n",
    "#### In this block we load Velocity data ####\n",
    "#### In this block we load Velocity data ####\n",
    "#### In this block we load Velocity data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>azim</th>\n",
       "      <th>azimStd</th>\n",
       "      <th>delMLT</th>\n",
       "      <th>goodFit</th>\n",
       "      <th>MLAT</th>\n",
       "      <th>normMLT</th>\n",
       "      <th>vSaps</th>\n",
       "      <th>velSTD</th>\n",
       "      <th>endPtMLAT</th>\n",
       "      <th>endPtNormMLT</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_index</th>\n",
       "      <th>dst_bin</th>\n",
       "      <th>datetimeStr</th>\n",
       "      <th>AE</th>\n",
       "      <th>AL</th>\n",
       "      <th>AO</th>\n",
       "      <th>AU</th>\n",
       "      <th>count</th>\n",
       "      <th>maxCount</th>\n",
       "      <th>probOcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.278027</td>\n",
       "      <td>3.056854</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>56.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>439.581783</td>\n",
       "      <td>42.430720</td>\n",
       "      <td>56.51</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>(-75, -50]</td>\n",
       "      <td>20131002-08-26</td>\n",
       "      <td>552</td>\n",
       "      <td>-285</td>\n",
       "      <td>-9</td>\n",
       "      <td>267</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>0.150376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.278027</td>\n",
       "      <td>3.056854</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>56.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>439.581783</td>\n",
       "      <td>42.430720</td>\n",
       "      <td>56.51</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>(-75, -50]</td>\n",
       "      <td>20131002-08-26</td>\n",
       "      <td>552</td>\n",
       "      <td>-285</td>\n",
       "      <td>-9</td>\n",
       "      <td>267</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>0.150376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-13.713678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>56.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>782.837900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.69</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>(-75, -50]</td>\n",
       "      <td>20130525-06-54</td>\n",
       "      <td>401</td>\n",
       "      <td>-155</td>\n",
       "      <td>46</td>\n",
       "      <td>246</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>0.150376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-13.713678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>56.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>782.837900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.69</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>(-75, -50]</td>\n",
       "      <td>20130525-06-54</td>\n",
       "      <td>401</td>\n",
       "      <td>-155</td>\n",
       "      <td>46</td>\n",
       "      <td>246</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>0.150376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.651055</td>\n",
       "      <td>4.944216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>56.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>501.724761</td>\n",
       "      <td>24.297278</td>\n",
       "      <td>56.40</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>(-75, -50]</td>\n",
       "      <td>20130525-06-56</td>\n",
       "      <td>389</td>\n",
       "      <td>-162</td>\n",
       "      <td>33</td>\n",
       "      <td>227</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>0.150376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        azim   azimStd  delMLT goodFit  MLAT  normMLT       vSaps     velSTD  \\\n",
       "0  -1.278027  3.056854     1.0    True  56.5      0.0  439.581783  42.430720   \n",
       "1  -1.278027  3.056854     1.0    True  56.5      0.0  439.581783  42.430720   \n",
       "2 -13.713678       NaN     NaN   False  56.5      0.0  782.837900        NaN   \n",
       "3 -13.713678       NaN     NaN   False  56.5      0.0  782.837900        NaN   \n",
       "4  11.651055  4.944216     1.0    True  56.5      0.0  501.724761  24.297278   \n",
       "\n",
       "   endPtMLAT  endPtNormMLT    ...    dst_index     dst_bin     datetimeStr  \\\n",
       "0      56.51         -0.44    ...        -67.0  (-75, -50]  20131002-08-26   \n",
       "1      56.51         -0.44    ...        -72.0  (-75, -50]  20131002-08-26   \n",
       "2      56.69         -0.76    ...        -53.0  (-75, -50]  20130525-06-54   \n",
       "3      56.69         -0.76    ...        -58.0  (-75, -50]  20130525-06-54   \n",
       "4      56.40         -0.49    ...        -53.0  (-75, -50]  20130525-06-56   \n",
       "\n",
       "    AE   AL  AO   AU count  maxCount   probOcc  \n",
       "0  552 -285  -9  267    20       133  0.150376  \n",
       "1  552 -285  -9  267    20       133  0.150376  \n",
       "2  401 -155  46  246    20       133  0.150376  \n",
       "3  401 -155  46  246    20       133  0.150376  \n",
       "4  389 -162  33  227    20       133  0.150376  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out some values where number of percent datapoints are pretty low.\n",
    "# We'll not use a number but divide data into different Dst groups and\n",
    "# discard locations where number of points are relatively low.\n",
    "# Get max points at a given Lat, MLT, DstBin\n",
    "################## NEW METHOD ##################\n",
    "################## NEW METHOD ##################\n",
    "################## NEW METHOD ##################\n",
    "dstSapsMLTLatCountDF = velsDataDF.groupby([\"dst_bin\", \"normMLT\", \"MLAT\"]).size().reset_index()\n",
    "maxCntMLTLatDst = dstSapsMLTLatCountDF.groupby([\"dst_bin\"]).max().reset_index()\n",
    "maxCntMLTLatDst = maxCntMLTLatDst.drop([\"normMLT\", \"MLAT\"], 1)\n",
    "maxCntMLTLatDst.columns = [\"dst_bin\", \"maxCount\"]\n",
    "dstSapsMLTLatCountDF = pandas.merge( dstSapsMLTLatCountDF, maxCntMLTLatDst, \\\n",
    "                              on=[\"dst_bin\"], how='inner')\n",
    "dstSapsMLTLatCountDF.columns = [\"dst_bin\", \"normMLT\", \"MLAT\", \"count\", \"maxCount\"]\n",
    "dstSapsMLTLatCountDF[\"probOcc\"] = dstSapsMLTLatCountDF[\"count\"]/dstSapsMLTLatCountDF[\"maxCount\"]\n",
    "# Filter out unwanted values\n",
    "dstSapsMLTLatCountDF = dstSapsMLTLatCountDF[ \\\n",
    "                        dstSapsMLTLatCountDF[\"probOcc\"] >= perCutoffMLTMLAT\\\n",
    "                        ].reset_index(drop=True)\n",
    "velsDataDF = pandas.merge( velsDataDF, dstSapsMLTLatCountDF,\\\n",
    "                          on=[\"dst_bin\", \"normMLT\", \"MLAT\"], how='inner' )\n",
    "velsDataDF.to_csv(\"../data/processed-vels-geomag-extra.txt\", sep=' ', index=False)\n",
    "################## NEW METHOD ##################\n",
    "################## NEW METHOD ##################\n",
    "################## NEW METHOD ##################\n",
    "\n",
    "################## OLD METHOD ##################\n",
    "################## OLD METHOD ##################\n",
    "################## OLD METHOD ##################\n",
    "# countDF = velsDataDF.groupby([ \"normMLT\", \"MLAT\" ]).size().reset_index()\n",
    "# countDF.columns = [ \"normMLT\", \"MLAT\", \"count\" ]\n",
    "# # Choose only columns which have atleast 100 points\n",
    "# countDF = countDF[ countDF[\"count\"] >= numPointsCutoffMLTMLAT ].reset_index(drop=True)\n",
    "# # Merge with velsDataDF to filter out unwanted values\n",
    "# velsDataDF = pandas.merge( velsDataDF, countDF,\\\n",
    "#                           on=[\"normMLT\", \"MLAT\"], how='inner' )\n",
    "# velsDataDF.to_csv(\"../data/processed-vels-geomag-extra.txt\", sep=' ', index=False)\n",
    "# print velsDataDF.columns.tolist()\n",
    "################## OLD METHOD ##################\n",
    "################## OLD METHOD ##################\n",
    "################## OLD METHOD ##################\n",
    "velsDataDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
